\section{Related work}
\label{sec:related_work}

\paragraph{Traffic analysis methods}
Tor's threat model excludes global adversaries~\cite{Dingledine2004a}, but the
practical threat of such adversaries is an important question that academia has
spent considerable effort on answering.  In 2004, when the Tor network counted
only 33 relays, Feamster and Dingledine investigated the practical threat that
network-level adversaries pose to anonymity networks~\cite{Feamster2004a}.  In
particular, the authors considered an attacker that controls an autonomous
system that is traversed both for ingress and egress traffic, allowing the
attacker to correlate both streams.  Using AS path prediction~\cite{Gao2001a},
Feamster and Dingledine found that powerful tier-1 ISPs reduce location
diversity of anonymity networks.

In 2007, Murdoch and Zieli\'{n}ski drew attention to IXP-level adversaries, a
class of adversaries that was missing in Feamster and Dingledine's
work~\cite{Murdoch2007a}.  Murdoch and Zieli\'{n}skishowed showed that IXP
adversaries are able to correlate traffic streams, even in the presence of
packet sampling rates as low as one in 2,000.

In 2013, Johnson et al.~\cite{Johnson2013a} presented the first large-scale
study on the risk of Tor users facing relay-level and network-level
adversaries.  The authors developed a Tor path simulator (TorPS~\cite{TorPS})
that simulates Tor circuits for a number of user models the authors developed.
By combining TorPS with AS path prediction, Johnson et al. could answer
questions such as the average time until a Tor user's circuit is linked
together by an AS or IXP.

In 2015, Juen et al.~\cite{Juen2015a} questioned the accuracy of path
prediction algorithms that prior work~\cite{Johnson2013a,Feamster2004a} used to
estimate the threat of correlation attacks.  The authors compared AS path
predictions to millions of traceroutes they initiated from 25\% of Tor relays
by bandwidth at the AS level.  Only 20\% of predicted paths matched the paths
observed in traceroute, calling into question the results of prior work.  A
limitation of Juen et al.'s work is that they could not consider the reverse
path in traceroutes.  This shortcoming was addressed in 2015 by Sun et
al.~\cite{Sun2015a}.  While past work treated routing as static, Sun et al.
leveraged the dynamic nature of routing to show that network adversaries are a
bigger threat than thought.  Most recently in 2016, Nithyanand et
al.~\cite{Nithyanand2016a} used AS path prediction to evaluate the practical
threat faced by users in the top 10 countries using Tor.

We improve on previous work in two significant ways; (\emph{i}) we are the
first to consider the DNS protocol for traffic analysis and evaluate its
practical threat, and (\emph{ii}) we propose a method to scale the measurement
method proposed by Juen et al.~\cite{Juen2015a}.  Our method leverages the
volunteer-run RIPE Atlas measurement platform~\cite{atlas} instead of
convincing relay operators to run third-party scripts.  This allows us to fully
automate our method and achieve previously unprecedented scale.

\paragraph{Website fingerprinting}
In 2009, Hermann, Wendolsky, and Federrath~\cite{Hermann2009a} demonstrated the
first website fingerprinting attack against anonymity systems---including
Tor---in a closed-world setting.  In 2011, Panchenko et
al.~\cite{Panchenko2011a} greatly improved on Hermann et al.'s detection rate
and provided insight into an open-world setting.  In 2012, Cai et
al.~\cite{Cai2012a} improved on prior work by proposing an attack that used
Hidden Markov Models to determine if a sequence of page requests all come from
the same site.  The authors used an open-world setting for their evaluation.

Wang and Goldberg~\cite{Wang2013a} proposed an improved attack that employed a
new method for data gathering.  Instead of working with TCP segments, the
authors extracted Tor cells from packet traces and managed to remove flow
control cells from the traces.  In 2014, Wang et al.~\cite{Wang2014a} further
improved on their results.

Cai et al.~\cite{Cai2014b} analyzed what traffic features provide the most
predictive power, proved a lower bound of any defense that achieves a certain
level of security, and provided a framework to investigate the performance of
fingerprinting attacks.

Juarez~\cite{Juarez2014a} critically evaluated past fingerprinting attacks,
showing that they all made numerous simplifying assumptions.  The authors
suggest that fingerprinting attacks are still difficult to run outside a lab
setting as an attacker will have to consider operating system differences, page
changes, and background traffic.  Most recently in 2016, Panchenko et
al.~\cite{Panchenko2016a} showed that web\emph{page} fingerprinting lacks
precision in the open world while web\emph{site} fingerprinting remains
practical.

\fixme{What are we doing better than past work?}
