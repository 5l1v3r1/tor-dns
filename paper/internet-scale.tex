\section{Internet-scale analysis}
\label{sec:internet-scale}
After having presented and evaluated our attack in an isolated setting, we are
left with one final question.  How does the attack impact the entirety of Tor
users?  We answer this question by combining and improving state-of-the-art
techniques, illustrated in Figure~\ref{fig:simulations}.  We model the activity
of Tor users and simulate corresponding Tor path selection using
TorPS~\cite{TorPS}.  TorPS returns guard and exit relays, which we then feed as
input---together with source and destination addresses---into our framework that
runs traceroutes from RIPE Atlas nodes.  The following subsection discuss our
setup in detail.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/simulations.pdf}
	\caption{The relation among our simulation components.  Our goal is to
	determine the ASs a Tor user's traffic traverses into and out of the Tor
	network.  Duplicate ASs on both sides can deanonymize streams.}
	\label{fig:simulations}
\end{figure}

In this analysis we assume that an AS that can see traffic entering the Tor network and 
DNS traffic at the other end can use this information in order to deanonymize the 
entering traffic. Here we seek to measure the chances an AS will be in the ``correct'' 
position to carry out such an attack.

A Tor exit can carry out DNS name resolution in two different ways: it can either do it
itself by running a name server locally, or it can rely on a 3rd-party name server, 
such as its ISP's name server or a public DNS name server like Google's 8.8.8.8.

Let us examine these two different scenarios: local resolution vs. 3rd-party 
resolution. With local resolution, a good position for an AS is to be on the path between 
a Tor client and a Tor guard and on the path between a Tor exit and any of the name 
servers the exit has to communicate with in order to resolve the name. These name servers 
can include the root name server, the TLD servers, and the other ones**fix this. The ASes 
along the way from the exit relay to the name servers will be able to see the domain 
names that are being queried.

For 3rd party resolution, a good position for an AS is to be on the path between 
a Tor client and a Tor guard and on the path between the exit relay and that 3rd party 
name server. Beyond that, the queries will look like they're coming from the IP address 
of the name server and not the IP address of the exit relay, which is what we're interested 
in.

In order to do our analysis, we need to have some way of figuring out the AS-level paths. 
We considered two options: the use of AS path inference and the use of traceroute data. 
We decided against AS path inference because Juen et al. showed that it can be quite
inaccurate~\cite{Juen2015a}.  Thus, we decided to go with actual traceroute data. Here,
again, we had to consider our options on how to obtain this data. We considered the
PlanetLab and RIPE Atlas measurement platforms and asking Tor relay operators to run
traceroutes for us.  We decided against using Tor relay operators to run traceroutes for
us because of the inconvenience (Murdoch paper did this and Juen paper did this). We
decided against PlanetLab because its nodes tend to be in university networks. Hence, we
decided to use RIPE Atlas~\cite{atlas}. RIPE Atlas has probes in many ASes that have Tor
relays. As shown in Table~\ref{tab:atlas-coverage}, for a day in May 2016, we found that
RIPE Atlas had probes in 52\% of ASes that contain Tor exit relays.  We found that RIPE
Atlas has probes in 51\% of ASes that contain Tor guard relays. More importantly, we found
that Atlas ASes cover 58\% of Tor exit bandwidth and 74\% of Tor guard bandwidth. This is
important because Tor relay usage is not evenly split among all of the relays. This
coverage is significant because as the Murdoch paper states, ``As Tor does not currently
implement a a mechanism for performing traceroutes, the operator of the node must do so
manually. Furthermore, the Juen paper was only able to cover around 28\% of exit bandwidth
by asking operators to run traceroutes for them.  Thus, we find that RIPE Atlas gives us a
great way to study the paths that Tor circuits might traverse. (I'm wondering if a table
might be better for those percentages .  . .) In order to get our traceroutes, we ran
traceroutes from probes that that were in the same ASes as Tor guards and exits. TODO:
Mention why we picked AS 6128 as our client AS.

% - 197 out of all 377 (52%) Tor exit ASs have Atlas probes.
% - 220 out of all 434 (51%) Tor guard ASs have Atlas probes.

% - Atlas ASs cover 57.53% of Tor exit bandwidth.
% - Atlas ASs cover 73.59% of Tor guard bandwidth.

\begin{table}[t]
	\caption{The coverage of RIPE Atlas nodes that are colocated with Tor guard and exit
	relays.}
	\label{tab:atlas-coverage}
	\centering
	\begin{tabular}{l|r r}
	\toprule
	\textbf{Atlas probe coverage} & \textbf{Tor guard ASs} & \textbf{Tor exit ASs} \\
	\midrule
	By bandwidth & 73.59\% & 57.53\% \\
	By number & 50.69\% & 52.25\% \\
	\bottomrule
	\end{tabular}
\end{table}

We also had to consider what method we'd use to translate the IP addresses in our 
traceroutes to ASes. The options we considered were Team Cymru and pyasn. We decided to 
go with pyasn because it uses actual control plane data in order to perform the 
translations vs. using whois, which is what Team Cymru uses (find source for this). 
(A word of caution: if there is an AS that is annoucing bogons on a particular day, and 
you unluckily happen to pick that day, you might get bogus translations. For example, an 
IP address like 10.10.10.1 might result in an AS number instead of 'None'.)

In order to measure the chances an AS will be in the ``correct'' position to carry out a
deanonymization attack, we make use of TorPS, the Tor Path Simulator. This tool 
``faithfully mimics the behavior of Tor client software for creating exit circuits.''
That is, it gives us realistic combinations of guards and exits based on the state of the 
Tor network for a given time. TorPS is based on Tor version X. For each sample, it uses 
one guard which will expire after 270 days. We have TorPS simulate the behavior of a Tor 
client for the month of March 2016. We use 50,000 samples for the same reasons given in
``Users Get Routed.'' (Should we mention the fixes we made to TorPS? Should we mention 
the problem we found with their user models?) For our simulation, we had each sample 
visit domain.com---a website under our control---once per hour for the entire
month of March 2016. Thus, each sample had a total of 744 opportunites to be
compromised (this assumes that the circuit changes every time the sample/user
visits domain.com). It is much better to use a tool like TorPS in order to
measure these chances instead of just using all permutations of guard and
exits.

In this analysis, we completely ignore caching of any type, so our results are overly 
pessimistic. TODO: Mention that these results apply only to what we had data for: about 
half.

Describe the four scenarios in the graph here: 1) Traditional, 2) Google, 3) Local, and 
4) Status Quo.

Traditional: ``Traditional'' represents the paths between all exit ASes that RIPE Atlas 
has coverage for and domain.com. That is, this measurement doesn't take DNS into account 
at all, as has been traditionally done.

Google (8.8.8.8): ``Google'' represents what would happen if all exit relays used 
Google's public DNS server for resolution. We ran traceroutes from probes in all the exit 
ASes that RIPE Atlas had coverage for to 8.8.8.8.

Local: ``Local'' represents what would happen if all exit relays ran their own name 
servers locally (e.g., the use of unbound) on their own machines. In order to figure out 
what ASes would be traversed on the way to resolve domain.com. In order to figure out the 
DNS delegation path, we use part of the ddptr tool. Assuming no caching and assuming 
iterative resolution, for domain.com the name server will have to visit three name servers:
a root name server, a top level domain name server, and a second level domain name server. 

Status Quo: ``Status Quo'' represents to the best of our ability the type of name resolution 
that exit relays perform, whether that be their own or the use of 3rd-party name servers, 
which again includes public DNS name servers and their ISPs' name servers. To figure this 
out, we used the exitmap tool, our own website, and our own authoritative name server 
in order to figure out the IP addresses of the name servers that the exit relays were 
using. We then performed traceroutes from the exit ASes to the appropriate IP addresses 
in order to get the AS-level paths.

TODO: Get the numbers from my data for the fan-out.

As you can see in Figure~\subref{fig:compromised-streams} and
Figure~\subref{fig:time-until-compromise}, 
Google fared much better than the other situations. This might lead you to believe that 
Tor exit relays should use 8.8.8.8 for all of their name resolution needs, but then you've 
given Google A LOT of power. It might actually be better to tell Tor exit relay operators 
to use their ISPs' default name servers because the ISPs get to see the Tor destinations 
anyway!

The Status Quo results are significantly better than the Local results because only X\% 
of Tor exit relays actually do their own resolution.

\begin{figure}[t]
\centering
\subfigure[The number of compromised streams for March 2016.]{
	\includegraphics[width=0.466\linewidth]{figures/num-compromised-streams.pdf}
    \label{fig:compromised-streams}
}
\subfigure[The time to first compromise for March 2016.]{
	\includegraphics[width=0.466\linewidth]{figures/time-until-compromised.pdf}
    \label{fig:time-until-compromise}
}
\caption{Two simulations for March 2016 that show the number of compromised
	streams (Figure~\subref{fig:compromised-streams}) and the time until the
	first compromised stream (Figure~\subref{fig:time-until-compromise}).}
\label{fig:compromise-stream-time}
\end{figure}
