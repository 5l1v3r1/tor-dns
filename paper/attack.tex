\section{WF+DNS Attacks}
\label{sec:attack}

As with conventional correlation attacks, an attacker must observe
traffic that is both entering and exiting
the Tor network; in contrast to threat models from previous work, we
incorporate DNS instead of only
TCP traffic.
Figure~\ref{fig:attack-scenario} illustrates our correlation attack; it requires the
following building blocks:
\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/attack-scenario.pdf}
	\caption{An overview of our correlation attack.  An adversary must monitor
		both ingress and egress traffic.  A network-level adversary between the
		client and its guard monitors ingress traffic.  The same adversary
		monitors egress traffic between the exit and a DNS server, or the DNS
		server itself.  Both ingress (encrypted Tor traffic) and egress (DNS
		requests) traffic then serve as input to a Website fingerprinting
		attack.}
	\label{fig:attack-scenario}
\end{figure}

\begin{itemize}
    \item \emph{Ingress sniffing:} An attacker must observe traffic that is
		entering the Tor network.  The attacker can operate on the network level,
		as a malicious ISP or an intelligence agency.  In addition, the
		attacker can operate on the relay level by running a malicious Tor guard
		relay.  In both cases, the attacker can only observe encrypted
		data, so packet lengths and
		directions are the main inputs for website fingerprinting~\cite{Panchenko2016a}.
    \item \emph{Egress sniffing:} To observe both ends of the communication, an
		attacker must also observe egress DNS traffic.  We expect the adversary
		either to be on the path between exit relay
		and a DNS server or to run a malicious DNS
		resolver or server.  An attacker may also run an exit relay,
		but in this case, conventional end-to-end correlation
                attacks are equally effective as those we describe here.
\end{itemize}
We combine a conventional WF attack operating on traffic from ingress sniffing with
DNS traffic observed by egress sniffing, creating WF+DNS attacks. Our attacks
correlate the web\emph{sites} observed by the WF attack in ingress traffic with
the web\emph{sites} identified from DNS traffic. Next, we describe how we
simulate the DNS traffic from Tor exits, how we map DNS requests to websites,
and finally present our two WF+DNS attacks.

\subsection{Approximating DNS traffic from Tor exits}
\label{sec:attack:sim}

We first investigate the DNS traffic that Tor's exit relays send.
We cannot use real data because there are no logs of outgoing traffic
from Tor exit relays available to us and ethical considerations kept us
from trying to collect them (\eg, by operating exit relays and recording
the outgoing traffic). We therefore opt to approximate the DNS traffic
emerging from Tor exit relays by building a model of a typical Tor
user's website browsing patterns, as well as the effects of DNS caching.

\subsubsection{Modeling which pages Tor users visit}
\label{sec:attack:pop}

We first build a model to approximate {which websites} Tor users visit.
Currently, there are about 173 million active
websites~\cite{numberofwebsites}; the Alexa ranking~\cite{alexatop1k}
gives insights into their popularity based on the browsing behavior of
a sample of all Internet users. The distribution of the popularity of
these websites has previously been fit to a power-law distribution based
on the rank of the
website~\cite{DBLP:journals/network/MahantiCMAW13,ClausetSN09,AliS07}.
For the page-view numbers of the Alexa top 10,000 websites, we found a
power-law distribution to be a good fit as neither a lognormal nor a
power-law distribution with exponential cutoff (truncated power-law
distribution) offered significantly better fits\footnote{We used the
		python power-law package~\cite{power-law}
		%\url{https://github.com/jeffalstott/power-law}
		for fitting. After manual adjustment to make a conservative
		model (worst for the attacker), the resulting power-law
		distribution had an $\alpha$ parameter of about $1.1349$. Note
		that page view numbers as collected by Alexa ignore multiple
		visits by the same user on the same day (see
		\url{https://support.alexa.com/hc/en-us/articles/200449744}), so
		the ranking might be slightly off when modelling website visit
		patterns.}.
Thus, we use a power-law distribution to model which websites are
visited by Tor users. This might overestimate the popularity of
higher-ranked websites, because Tor users might be visiting less popular
websites, such as websites that are censored in different parts of the
world more often than a typical Internet user. We will discuss the
implications of our model for browing behavior in later sections.

\subsubsection{Modeling how often Tor users visit each page}
\label{sec:load-freq}
% phw's numbers extrapolated
Next, we determined how many websites Tor users visit in a certain time span.
We approximated this number by seting up an exit relay whose exit policy
included only ports 80 and 443, so our relay would only forward web traffic.  We
then used tshark to capture the timestamps of DNS requests---but no DNS
responses.  We made sure that our tshark filter did not capture packet payloads
or headers, so we were unable to learn what websites Tor users were visiting.
In addition, we patched {\tt tshark} to log timestamps at a five-minute
granularity. The coarse timing granularity allows us to publish this
dataset with minimal privacy implications;
Section~\ref{sec:ethics} discusses the ethical implications of this
experiment in more detail.  We ran the experiment for approximately two weeks
from May 15, 2016 to May 31, 2016, which allowed us to determine the number of
DNS requests for 4,832 five minute intervals.
Figure~\ref{fig:dns-reqs} shows this timeseries.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/dns-reqs.pdf}
	\caption{The number of DNS requests per five minute intervals on our
	exit relay.  Using a privacy-preserving measurement method, we only
	determined approximate timestamps and no content.  The red line at $y = 105$
	illustrates the median of the distribution.}
	\label{fig:dns-reqs}
\end{figure}

We then interpolate these numbers to all Tor exit relays based on the published
bandwidth statistics of all Tor exit relays. We found our exit relay to have an
average of 119.3 outgoing DNS requests per five minutes during a two-week period,
which corresponds to about 1.583 page visits per minute (taking caching into
\fixme{did this use the old power-law distribution?}
account and assuming a power-law distribution of site popularity as described
above).  We configured exit relay to only allow port 80 and 443 during the time
of the measurements to avoid counting DNS requests from protocols other than
HTTP and HTTPS. We then used the self-reported bandwidth information from Tor
exit relays collected in the ``extra-info'' descriptors available on
CollecTor~\cite{collector} to estimate the number of page loads on each of the
about 1,200 exit relays active at that time.

\subsubsection{Modeling the effects of DNS caching}
To analyze which DNS requests the adversary can see, we need to
take caching of DNS responses into account. We ignore client-side DNS
caching since it is disabled by default, as described in
Section~\ref{sec:background}.
% From Tobias: I manually could not get Firefox to cache anything,
% not even between pageloads on the same site (went to kau.se, waited 20s,
% then clicked on a link: little tor client-side still got a DNS response
% from the exit for kau.se.)
On the exit relays, which perform DNS resolution on behalf of Tor clients, caching
is relevant, because the requests of all users connected to an exit
share the same cache. An exit
relay maintains its own DNS cache\footnote{Around eventdns.c.  See the
		code in src/or/dns.c.} and enforces a minimum TTL of 60 seconds
and a maximum TTL of 30 minutes\footnote{See src/or/dns.c:278}.  We
refer to this as Tor's \emph{TTL clipping}. Due to a
bug in Tor that we identified\footnote{We redacted the link to the bug report to anonymize our paper
submission.},
% \url{https://bugs.torproject.org/19025}}
the TTL of all DNS responses are set to 60 seconds.

%\subsubsection{Sliding window approach to compensate for DNS caching}
% we ignore caching by having a window of X minutes
If a user of an exit relay requests the IP address for a domain name
that has been cached by the exit relay before (and the cache entry is
not expired yet), then the adversary will not be able to observe an
outgoing DNS request for this domain name. But the adversary can
recorded all DNS requests from the exit relay in the past $x$ seconds,
where $x$ is the maximum TTL value (that is, maintain a sliding window of
length $x$) to obtain a list of all possibly requested domain names at the
given point in time. A domain name that is requested by a client at the
given point in time is either contained in the cache or not. If it is
not contained in the cache, it will be observable as a new, outgoing DNS
request from the exit relay. If it is contained in the cache it must
have been resolved by the exit relay in the last $x$ seconds and will
therefore be contained in the sliding window.

We assume that an adversary applies this sliding window technique and
model the observable DNS data accordingly.
The attacker observes a fraction of Tor exit bandwidth,
for a specific window length,
and together with our page load frequency estimation
this triggers a number of page loads in our simulation.
For each page load event we randomly draw a website using the
power-law website popularity distribution described above and put its
DNS requests into the window. As we will see next, we do not need to
simulate or consider the fact that the observed fraction of Tor exit bandwidth
corresponds to many different exits with individual caches.

\subsection{Inferring website visits from DNS requests}
\label{sec:dns2site}

Given a sliding window of DNS requests, we investigate
how this information can help determine whether a user has visited a website of interest.
In April 2016, we visited the Alexa top one million websites five times
and collected all DNS requests generated by visiting each
website. We refer to the data collected for one visit as a \emph{sample}.
We performed these measurements in rounds from a university network, where
each round browsed all one million websites in a random order before visiting
the same website again. We used Tor Browser Bundle (TBB)~5.5.4
configured to {not browse over Tor}: TBB ensures that the browser behavior
is identical to a TBB user over Tor. Not using Tor bypasses
IP blacklists and CAPTCHAs triggered by IP addresses of
Tor-exits~\cite{Khattak2016a}.
Table~\ref{tab:dns-censor} shows the percentage of websites in our dataset that
risks censorship by CloudFlare or Akamai if collecting data over Tor, as
identified by Khattak \ea~\cite{Khattak2016a}. We also include Google,
which is prevalent in our dataset and
reportedly restricts access to Tor users (when searching).

\begin{table}[t]
	\centering
	\begin{tabular}{l r}
	\toprule
	\textbf{Description} & \textbf{Percentage} \\
	\midrule
	Website behind CloudFlare IP & 6.44 \\
	Domain on website uses CloudFlare & 25.81 \\
	Domain on website uses Akamai & 33.86 \\
	Domain on website uses Google & 77.43 \\
	\bottomrule
	\end{tabular}
	\caption{The percentage of websites on Alexa top-1 million websites using providers
	involved in censoring or restricting access from Tor~\cite{Khattak2016a}.}
	\label{tab:dns-censor}
\end{table}

We collected 2,540,941 domain names over 60,828,453 DNS
requests. The dataset contains 2,260,534 domains that are unique to
a particular website; we call these domains {\em unique
  domains}. Unique domains are particularly interesting because a DNS
lookup to a unique domain during an HTTP session can identify that a
user has visited a specific website. Figure~\ref{fig:unique-domains} shows the
fraction of
sites with unique domains for groups of websites in the Alexa top one million.
For 96.8\% of all sites there exists at least one unique domain.
Interestingly, more
popular websites are less likely have a unique domain associated with
them.
Table~\ref{tab:dns-domains} shows statistics for the number of DNS domains per
website. At least half of the sites we have ten domains per website where
two of them are unique, suggesting that many website visits can be
identified from a single DNS request.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/dns-unique-domains.pdf}
	\caption{The fraction of websites on Alexa top one million that have at least
	one unique domain. The vast majority of sites (96.8\%) have
        unique domains. \protect\xxx{NF: if this is cumulative, then why is there a
          dip around 30?}}
	\label{fig:unique-domains}
\end{figure}

\begin{table}[t]
	\centering
	\begin{tabular}{l r r r r}
	\toprule
	\textbf{DNS Domains} & \textbf{Median} & \textbf{Mean} & \textbf{Min} & \textbf{Max} \\
	\midrule
	per site & 10 & $12.2\pm11.2$ & 1 & 397 \\
	unique per site & 2 & $2.3\pm\phantom{0}1.8$ & 0 & 363 \\
	\bottomrule
	\end{tabular}
	\caption{The mean, median, standard deviation, minimum and
          maximum number of DNS domains per website in the Alexa top 1
          million. More than half of the sites have two DNS domains that
        are unique to that site.}
	\label{tab:dns-domains}
\end{table}


\begin{table}[t]
\centering
\begin{tabular}{l r r}
\toprule
\textbf{TTLs (s)} & \textbf{Median} & \textbf{Mean} \\
\midrule
% 2016/04/28 15:52:39 DNS records TTL mean 9780.0, std 42930.5, median 255.0, min 0.0, max 604800.0
raw & 255 & $9780.0\pm42930.5$ \\ % & 0 & 604800 \\
% 2016/04/28 15:44:05 DNS records TTL mean 701.5, std 755.3, median 255.0, min 60.0, max 1800.0
Tor & 255 & $701.5\pm\phantom{00}755.3$ \\ %& 60 & 1800 \\
% 2016/04/28 15:52:39 	unique domain TTL mean 13022.2, std 35054.4, median 900.0, min 0.0, max 604800.0
unique raw & 900 & $13022.2\pm35054.4$ \\ %& 0 & 604800 \\
% 2016/04/28 15:44:05 	unique domain TTL mean 1005.3, std 789.6, median 900.0, min 60.0, max 1800.0
unique Tor & 900 & $1005.3\pm\phantom{00}789.6$ \\ %& 60 & 1800 \\
% 2016/04/28 15:52:39 	unique domain _min_ TTL mean 3833.9, std 11073.6, median 60.0, min 0.0, max 604800.0
min unique raw & 60 & $3833.9\pm11073.6$ \\ % & 0 & 604800 \\
% 2016/04/28 15:44:05 	unique domain _min_ TTL mean 644.2, std 763.8, median 60.0, min 60.0, max 1800.0
min unique Tor & 60 & $644.2\pm\phantom{00}763.8$ \\ %& 60 & 1800 \\
\bottomrule
\end{tabular}
\caption{Raw TTLs are unprocessed. Tor TTLs adhere to Tor's TTL clipping.
The unique prefix is for the TTL of unique domains while min unique only
considers the unique domains with the minimum TTL for each website.}
\label{tab:ttls}
\end{table}


Table~\ref{tab:ttls} shows the TTL of DNS records in our dataset
for the TTL as-is (raw) and when clipped by Tor\footnote{We simulate the
correct clipping despite the previously mentioned bug.}, for unique domains,
and when
only considering the unique domain for each website with the lowest TTL.
About half of the websites on Alexa top one million contain at
least one unique domain with a TTL of 60 seconds. Tor's TTL clipping has no
effect on the median TTL, but significantly reduces the mean TTL.

%\xxx{NF: *** stopped here. ***}

To evaluate the feasibility of mapping DNS requests to websites, we
construct a na\"{\i}ve website classifier that maps the unique domains
in a set of DNS requests to the corresponding website that contains a
matching set of domains.  With five-fold cross-validation on our Alexa
top one million dataset (with five samples per site),
we consider a closed world and an open world.
In the closed world, the attacker can use samples
from all sites in training; in the open world, some sites
are unmonitored and therefore unknown (as per the fold).  The
closed-world evaluation yields 0.955 recall.  In the open-world
evaluation, we monitor the Alexa top 500,000 with five samples each and
consider 433,000 unmonitored sites.  The number of unmonitored sites is
determined by our power-law
distribution to represent a relastic base rate (for the entire Tor network)
for evaluating our classifier: on average, for sites on Alexa top 500,000
to be visited 2.5 million times there will be about 433,000 visits to sites
outside of Alexa top 500,0000.  Our classifier does not take into account the
popularity of websites. 
The open-world evaluation yields a
recall of 0.947 for a precision of 0.984.  By accounting for the order
of requests, per-exit partitioning of DNS requests, TTLs, and website
popularity, we expect that classifying website visits from DNS requests
might be made even more accurate.

\xxx{NF: don't understand this paragraph.}
The closed-world setting
is also realistic when evaluating a classifier that maps DNS requests to
websites since gathering requests made by all 173 million active websites on the
Internet is practical with modest resources.
We use our conservative open-world results when simulating the Tor network for
evaluating WF+DNS attacks but
note that our results indicate that observing DNS requests in Tor is
largely equivallent to observing sites visited over Tor.

\subsection{Classifiers for WF+DNS attacks}

\xxx{NF: This section is also confusing.  See comments in Slack.}
We use Wa-kNN from Wang \ea~\cite{Wang2014a} and a list of sites derived from
observing DNS requests to implement two attacks:

\begin{description}
	\item[\texttt{ctw}] we ``close the world''
	on a Wa-kNN classifier that we modified to consider only the distance to observed
	sites when calculating the $k$-nearest neighbors. The classifier still
	considers the distance to all open instances to capture unmonitored sites.
	\item[\texttt{hp}] when Wa-kNN classifies a trace as a monitored site, confirm
	that we observed the same site in the DNS data (ensuring {\em high precision}). If
	not, make the final classification unmonitored.
\end{description}
\noindent
These approaches are generic and apply to any WF attack. The
\texttt{ctw} attack increases the effectiveness of conventional WF
attacks by making them more akin to
a closed-world setting, where websites have known
fingerprints.
Conceptually, the attack could also include
a custom weight-learning run---training only on observed sites---but our initial
results noted little to no gain, despite significant increases in
testing time.
We expect that this results because some features are more useful than others,
regardless of the training data~\cite{kfingerprinting}. The \texttt{hp} attack
only produces a positive classification if both ingress and egress
traffic are consistent.
