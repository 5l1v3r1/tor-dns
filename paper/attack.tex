\section{Attack overview}
\label{sec:attack}
We begin by summarizing our attack.  Our attack is based on traffic correlation,
so it requires an attacker to observe traffic that is both entering and exiting
the Tor network.  In contrast to earlier work, we consider DNS instead of just
end-to-end TCP packets.

Our attack is illustrated in Figure~\ref{fig:attack-scenario} and requires the
following building blocks:

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/attack-scenario.pdf}
	\caption{An overview of our correlation attack.  Ingress traffic is
	monitored either by a network-level adversary or the guard relay.  Egress
	traffic is monitored either by a network-level adversary or a DNS server.
	Captured DNS queries then serve as the candidate set for a Website
	fingerprinting attack.}
	\label{fig:attack-scenario}
\end{figure}

\begin{description}
	\item[Ingress sniffing] An attacker must observe traffic that is entering
		the Tor network.  The attacker can operate on the network level, i.e.,
		be a malicious ISP, or an intelligence agency.  In addition, the
		attacker can operate on the relay level, i.e., run a malicious Tor guard
		relay.  Note that in both cases, the attacker can only observe encrypted
		data.  Therefore, packet meta information such as packet lengths and
		directions serve as input to a website fingerprinting
		attack~\cite{Panchenko2016a}.
	\item[Egress sniffing] To observe both ends of the communication, an
		attacker must also observe egress DNS traffic.  We expect the adversary
		to operate on the network level, i.e., be on the path between exit relay
		and a DNS server.  Alternatively, the attacker can run a malicious DNS
		resolver or server.  Note that an attacker may also run an exit relay,
		but in that case she might as well do classical end-to-end correlation.
	\item[WF fingerprinting] We employ a website fingerprinting attack to
		determine if any of the recently observed DNS queries in egress traffic
		could be part of the encrypted ingress traffic.  Note that the DNS query
		itself does not tell us what \emph{page} a user is going to.
\end{description}

\subsection{Egress sniffing -- simulating Tor-exits' DNS traffic}
% the angle is: we have to simulate all DNS traffic from all Tor exits,
% the goal is to convince that we'ev made reasonable assumptions
To estimate the capability of an attacker, we need to investigate what
DNS data is observable for the attacker, that is what DNS request are
emerging from Tor exit relays.

We cannot use real data because there are no logs of outgoing traffic
from Tor exit relays available to us and ethical considerations kept us
from trying to collect them (\eg by operating exit relays and recording
the outgoing traffic). We therefore \emph{simulate} the DNS traffic
emerging from Tor exit relays.

% powerlaw, not uniform
First, we model \emph{what websites} are visited by Tor users.
Currently, there are about 170 billion active
websites\footnote{\url{http://news.netcraft.com/archives/2016/06/22/june-2016-web-server-survey.html}}
in the world and the Alexa ranking gives insights into their popularity
based on the browsing behaviour of a sample of all internet users
\footnote{\url{https://support.alexa.com/hc/en-us/articles/200449744}}.
It has been shown that the popularity of a website follows a powerlaw
distribution based on the rank of the website\fixme{citation needed}. So
we fit a powerlaw distribution to the page view numbers of the Alexa top
10\,000 websites\footnote{We used the python powerlaw package
		\url{https://github.com/jeffalstott/powerlaw} for fitting, the
		resulting powerlaw distribution had an $\alpha$ parameter of
		$1.10291152854$. Page view numbers as collected by Alexa ignore
		multiple visits by the same user on the same day, so the ranking
		might be slightly off for our purposes.} and use the result to
model which websites are visited by Tor users.
This might overestimate the popularity of higher-ranked websites because
Tor users might be visiting less popular websites, such as websites that
are censored in different parts of the world, more often than the
average internet users and we will discuss the implications of this for
our results later on.

% phw's numbers extrapolated
Second, we determine \emph{how many websites} are visited by Tor users in a
certain time span. We use (less intrusive) statistics on the number of
outgoing DNS requests collected on a Tor exit relay under our control
and interpolate these numbers to all Tor exit relays based on the
published bandwidth statistics of all Tor exit relays\footnote{We found
		our exit relay \texttt{746BE73579F274AFEFA5E12C4ADACD53784D0762}
		to have on average 119.3 outgoing DNS requests per 5 minutes
		during a two week period, which corresponds to about 1.583 page
		visits per minute (taking caching into account and assuming a
		powerlaw distribution of site popularity as described above). We
		then used the self-reported bandwidth information from Tor exit
		relays collected in the ``extra-info'' descriptors available on
		\url{https://collector.torproject.org/} to estimate the number
		of page loads on each of the about 1200 exit relays active at
		that time.}.

% TODO: bgre: continue here
% collected DNS requests for each Alexa 1M, able to simulate cache

% exit caching only (limitation: client-side caching negligible?)
% we ignore caching by having a window of X minutes

% The exit relay maintains its own caching layer around eventdns.c.
% See the code in src/or/dns.c. the DNS cache enforces a minimum TTL of
% 60 seconds and a maximum TTL of 30 minutes (see src/or/dns.c:278). We
% refer to this as Tor's \emph{TTL clipping}.

\subsubsection{DNS 1Mx5 dataset}
% TODO: pulls
% unique domain names are useful + TTL distribution (?)
TL;DR: unique domains are useful.

\subsubsection{A naive website classifier}
% TODO: whomever gets here first :-)
% we only look for unique domain names
% + naive, but already good enough for our purposes
% we note that we can significantly improve attacks by digging deeper
% into TTLs (even with window) and requests
In the end, we get a list of observed sites. We only look for sites that are
on the monitored list for the WF attack.
